{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Flickr8k metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dn = '/Users/pbos/projects/spokenLanguage/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flickr8k_dn = '/Users/pbos/projects/spokenLanguage/data/flickr8k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(flickr8k_dn + 'dataset.json') as fh:\n",
    "    metadata = json.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(metadata['images']) == 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{label: sum(image['split'] == label for image in metadata['images']) for label in ('train', 'test', 'val')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build small dataset\n",
    "\n",
    "Let's build a smaller dataset out of this.\n",
    "\n",
    "We need to make sure we include all types of images, i.e. training, testing and validation. We could select these out of the dataset to stay as close to flickr8k as possible. However, since it's just a training set, we can also just reset the metadata.\n",
    "\n",
    "In the original dataset, the fractions are 6:1:1 for train:test:val, so trying to stay close to that may make sense.\n",
    "\n",
    "In the original flickr1d build script (v1 and v2) we copied images and didn't actually use them. Now we use these images to rebuild the feature file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"flickr1h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_size = 100\n",
    "N_test = 12\n",
    "N_val = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dn = f'{base_dn}/{name}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_subdir = 'flickr_audio/'\n",
    "image_subdir = 'flickr8k_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(data_dn + image_subdir)\n",
    "os.makedirs(data_dn + audio_subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and image metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(mini_size):\n",
    "    shutil.copyfile(flickr8k_dn + '/Flickr8k_Dataset/Flicker8k_Dataset/' + metadata['images'][ix]['filename'],\n",
    "                    data_dn + image_subdir + metadata['images'][ix]['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniset_meta = {'dataset': name, 'images': metadata['images'][:mini_size]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range((mini_size - N_test - N_val), (mini_size - N_val)):\n",
    "    miniset_meta['images'][i]['split'] = 'test'\n",
    "for i in range((mini_size - N_val), mini_size):\n",
    "    miniset_meta['images'][i]['split'] = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i['split'] for i in miniset_meta['images']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dn + 'dataset.json', 'w') as fh:\n",
    "    json.dump(miniset_meta, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wav files -  wav2capt metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniset_img_filenames = [im['filename'] for im in miniset_meta['images']]\n",
    "with open(flickr8k_dn + 'wav2capt.txt', 'r') as fh:\n",
    "    wav2capt = [line.split() for line in fh if line.split()[1] in miniset_img_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dn + audio_subdir + 'wav2capt.txt', 'w') as fh:\n",
    "    for line in wav2capt:\n",
    "        fh.write(' '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniset_wav_filenames = [wav[0] for wav in wav2capt]\n",
    "with open(flickr8k_dn + 'wav2spk.txt', 'r') as fh:\n",
    "    wav2spk = [line.split() for line in fh if line.split()[0] in miniset_wav_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dn + audio_subdir + 'wav2spk.txt', 'w') as fh:\n",
    "    for line in wav2spk:\n",
    "        fh.write(' '.join(line) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then finally copy over the actual wavs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wv in wav2spk:\n",
    "    shutil.copyfile(flickr8k_dn + '/flickr_audio/wavs/' + wv[0],\n",
    "                    data_dn + audio_subdir + wv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platalea.utils.preprocessing as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.preprocess_flickr8k(data_dn, audio_subdir, image_subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove files\n",
    "\n",
    "We don't need the actual images and wavs after preprocessing into features, so we can safely remove them again before publishing our dataset somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(data_dn + image_subdir)\n",
    "for fn in glob.glob(data_dn + audio_subdir + \"*.wav\"):\n",
    "    os.remove(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add default config.yml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dn + 'config.yml', 'w') as fh:\n",
    "    fh.writelines([\"flickr8k_meta        dataset.json\",\n",
    "                   \"audio_features_fn    mfcc_features.pt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
